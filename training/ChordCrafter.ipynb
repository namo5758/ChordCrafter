{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47hAXBz0Vn0i"
      },
      "source": [
        "# Download datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YoUkZ1dOsAjJ",
        "outputId": "2bedae31-1983-4385-ed62-b498f43d5d5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-06-04 13:52:25--  https://huggingface.co/datasets/ailsntua/Chordonomicon/resolve/main/chordonomicon_v2.csv\n",
            "Resolving huggingface.co (huggingface.co)... 13.35.202.97, 13.35.202.40, 13.35.202.121, ...\n",
            "Connecting to huggingface.co (huggingface.co)|13.35.202.97|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs-us-1.hf.co/repos/32/19/3219792b72b684216fa25f158f3ef1a4e35d7672f255c70d6f5d5dc6705c53a4/9d2f4ccdc876a4e816712f128e4772b2af558c2a2923cce5be3a0364fbad9a8d?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27chordonomicon_v2.csv%3B+filename%3D%22chordonomicon_v2.csv%22%3B&response-content-type=text%2Fcsv&Expires=1749048745&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0OTA0ODc0NX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzMyLzE5LzMyMTk3OTJiNzJiNjg0MjE2ZmEyNWYxNThmM2VmMWE0ZTM1ZDc2NzJmMjU1YzcwZDZmNWQ1ZGM2NzA1YzUzYTQvOWQyZjRjY2RjODc2YTRlODE2NzEyZjEyOGU0NzcyYjJhZjU1OGMyYTI5MjNjY2U1YmUzYTAzNjRmYmFkOWE4ZD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoifV19&Signature=du6ZIUXho96Nx126Db8drqAXRahd-V6L60MTjDm6onXxAuFO31C56wz0-y0dGQV5BxU4KxRS%7EYAJiY7kGeU1ymug4BHaIicQgkPgCgJmEIoWC3TqtYH5JyGFW5UKHp%7Ev5fmYsNNlypXgDVdLa5P9ncVQ6aguckCmpfhPTcAFLpAFSPJpCsJFgrFNGtfC-ARGoxO2w8209cCIcKqLC8hJbxilTQc8UhQq6thD%7EUBuLRzKeVITaGO2S5cfmb3Dkn-jVhpHJ7LXhBxBKRX2yoBTT6XARF3M2KrEczPpReS5ImWPY7UiXlvo7jtxfLLxd%7ECUSch6OEpRAahxHhf-92worw__&Key-Pair-Id=K24J24Z295AEI9 [following]\n",
            "--2025-06-04 13:52:26--  https://cdn-lfs-us-1.hf.co/repos/32/19/3219792b72b684216fa25f158f3ef1a4e35d7672f255c70d6f5d5dc6705c53a4/9d2f4ccdc876a4e816712f128e4772b2af558c2a2923cce5be3a0364fbad9a8d?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27chordonomicon_v2.csv%3B+filename%3D%22chordonomicon_v2.csv%22%3B&response-content-type=text%2Fcsv&Expires=1749048745&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0OTA0ODc0NX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmhmLmNvL3JlcG9zLzMyLzE5LzMyMTk3OTJiNzJiNjg0MjE2ZmEyNWYxNThmM2VmMWE0ZTM1ZDc2NzJmMjU1YzcwZDZmNWQ1ZGM2NzA1YzUzYTQvOWQyZjRjY2RjODc2YTRlODE2NzEyZjEyOGU0NzcyYjJhZjU1OGMyYTI5MjNjY2U1YmUzYTAzNjRmYmFkOWE4ZD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSomcmVzcG9uc2UtY29udGVudC10eXBlPSoifV19&Signature=du6ZIUXho96Nx126Db8drqAXRahd-V6L60MTjDm6onXxAuFO31C56wz0-y0dGQV5BxU4KxRS%7EYAJiY7kGeU1ymug4BHaIicQgkPgCgJmEIoWC3TqtYH5JyGFW5UKHp%7Ev5fmYsNNlypXgDVdLa5P9ncVQ6aguckCmpfhPTcAFLpAFSPJpCsJFgrFNGtfC-ARGoxO2w8209cCIcKqLC8hJbxilTQc8UhQq6thD%7EUBuLRzKeVITaGO2S5cfmb3Dkn-jVhpHJ7LXhBxBKRX2yoBTT6XARF3M2KrEczPpReS5ImWPY7UiXlvo7jtxfLLxd%7ECUSch6OEpRAahxHhf-92worw__&Key-Pair-Id=K24J24Z295AEI9\n",
            "Resolving cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)... 18.155.68.103, 18.155.68.69, 18.155.68.50, ...\n",
            "Connecting to cdn-lfs-us-1.hf.co (cdn-lfs-us-1.hf.co)|18.155.68.103|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 264198044 (252M) [text/csv]\n",
            "Saving to: ‘chordonomicon_v2.csv’\n",
            "\n",
            "chordonomicon_v2.cs 100%[===================>] 251.96M   317MB/s    in 0.8s    \n",
            "\n",
            "2025-06-04 13:52:26 (317 MB/s) - ‘chordonomicon_v2.csv’ saved [264198044/264198044]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://huggingface.co/datasets/ailsntua/Chordonomicon/resolve/main/chordonomicon_v2.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXVtT1Y7WrUT"
      },
      "source": [
        "# Dataloader\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HyHMlbHlr9n_"
      },
      "outputs": [],
      "source": [
        "import os, re, math, json, random\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, random_split\n",
        "from tokenizers import Tokenizer\n",
        "from tokenizers.models import WordLevel\n",
        "from tokenizers.pre_tokenizers import Whitespace\n",
        "from transformers import (\n",
        "    PreTrainedTokenizerFast, AutoModelForCausalLM,\n",
        "    DataCollatorForLanguageModeling, TrainingArguments, Trainer\n",
        ")\n",
        "\n",
        "random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "TAG_REGEX = re.compile(r\"<[^>]+>\")\n",
        "\n",
        "def split_phrases(line: str):\n",
        "    if pd.isna(line): return []\n",
        "    return [p.strip() for p in TAG_REGEX.split(line) if p.strip()]\n",
        "\n",
        "def clean_tokens(phrase: str):\n",
        "    cleaned = re.sub(r\"[|,\\n]+\", \" \", phrase)\n",
        "    return re.sub(r\"\\s+\", \" \", cleaned).strip().split(\" \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OoA17OdasFjS"
      },
      "outputs": [],
      "source": [
        "class ChordDataset(Dataset):\n",
        "    \"\"\"CSV ('chords' column) → list[int] (token‑id)\"\"\"\n",
        "    def __init__(self, csv_path: str, max_len=256, add_eos=True):\n",
        "        df = pd.read_csv(csv_path, usecols=[\"chords\"]).drop_duplicates().dropna()\n",
        "\n",
        "        vocab, seqs = set(), []\n",
        "        for raw in df[\"chords\"]:\n",
        "            seq = []\n",
        "            for ph in split_phrases(raw):\n",
        "                seq.extend([t for t in clean_tokens(ph) if t])\n",
        "            if not seq: continue\n",
        "            if add_eos: seq.append(\"<eos>\")\n",
        "            seq = seq[:max_len]\n",
        "            vocab.update(seq)\n",
        "            seqs.append(seq)\n",
        "\n",
        "        specials = [\"<pad>\", \"<unk>\", \"<bos>\", \"<eos>\"]\n",
        "        self.token2idx = {tok: i for i, tok in enumerate(specials +\n",
        "                                                          sorted(vocab - set(specials)))}\n",
        "        self.idx2tok   = {i: t for t, i in self.token2idx.items()}\n",
        "\n",
        "        self.samples = [torch.tensor([self.token2idx[t] for t in s],\n",
        "                                     dtype=torch.long)\n",
        "                        for s in seqs]\n",
        "\n",
        "    def __len__(self):           return len(self.samples)\n",
        "    def __getitem__(self, idx):  return {\"input_ids\": self.samples[idx]}\n",
        "\n",
        "csv_path = \"chordonomicon_v2.csv\"\n",
        "raw_ds   = ChordDataset(csv_path, max_len=128)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "paMXVK7csJKP",
        "outputId": "88d53790-0d86-4756-ea94-5eb9d81fdc2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total 677199  | Train 541759 | Valid 67719 | Test 67721\n",
            "Vocab = 4264 tokens\n"
          ]
        }
      ],
      "source": [
        "n = len(raw_ds)\n",
        "n_train, n_val   = int(0.8*n), int(0.1*n)\n",
        "n_test           = n - n_train - n_val\n",
        "train_ds, val_ds, test_ds = random_split(raw_ds,\n",
        "    [n_train, n_val, n_test],\n",
        "    generator=torch.Generator().manual_seed(42)\n",
        ")\n",
        "\n",
        "print(f\"Total {n}  | Train {len(train_ds)} | Valid {len(val_ds)} | Test {len(test_ds)}\")\n",
        "print(f\"Vocab = {len(raw_ds.token2idx)} tokens\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IB_L5JjFWwd9"
      },
      "source": [
        "# Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ndg4D5dGsS9P"
      },
      "outputs": [],
      "source": [
        "tok_json = \"chord_tokenizer.json\"\n",
        "if not os.path.exists(tok_json):\n",
        "    tk = Tokenizer(WordLevel(raw_ds.token2idx, unk_token=\"<unk>\"))\n",
        "    tk.pre_tokenizer = Whitespace()\n",
        "    tk.save(tok_json)\n",
        "\n",
        "tokenizer = PreTrainedTokenizerFast(\n",
        "    tokenizer_file=tok_json,\n",
        "    pad_token=\"<pad>\", unk_token=\"<unk>\",\n",
        "    bos_token=\"<bos>\", eos_token=\"<eos>\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SpiZ2D7W1_3"
      },
      "source": [
        "# Create Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "referenced_widgets": [
            "a31a2431027b4543a4f62e91afa728a5",
            "d2339769d8844d71ab443d5241f5c226",
            "ec4f12a4ea4a4d6e93da8fc22cd16f46"
          ]
        },
        "id": "JIVUeZfrt1H3",
        "outputId": "3c4e1a5d-05e7-4644-ad9b-b93968554339"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a31a2431027b4543a4f62e91afa728a5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d2339769d8844d71ab443d5241f5c226",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ec4f12a4ea4a4d6e93da8fc22cd16f46",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": "GPT2LMHeadModel(\n  (transformer): GPT2Model(\n    (wte): Embedding(4264, 768)\n    (wpe): Embedding(128, 768)\n    (drop): Dropout(p=0.1, inplace=False)\n    (h): ModuleList(\n      (0-11): 12 x GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D(nf=2304, nx=768)\n          (c_proj): Conv1D(nf=768, nx=768)\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D(nf=3072, nx=768)\n          (c_proj): Conv1D(nf=768, nx=3072)\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head): Linear(in_features=768, out_features=4264, bias=False)\n)"
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
        "model.resize_token_embeddings(tokenizer.vocab_size)\n",
        "\n",
        "# ย่อ positional‑embedding เหลือ 128\n",
        "old_wpe = model.transformer.wpe.weight.data\n",
        "model.transformer.wpe = torch.nn.Embedding(128, old_wpe.size(1))\n",
        "model.transformer.wpe.weight.data = old_wpe[:128].clone()\n",
        "model.config.n_positions = model.config.n_ctx = 128\n",
        "\n",
        "\n",
        "model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y92qDT1_XBhm"
      },
      "source": [
        "# Train Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y5EP-FqOt4TZ"
      },
      "outputs": [],
      "source": [
        "collator = DataCollatorForLanguageModeling(\n",
        "    tokenizer=tokenizer, mlm=False, pad_to_multiple_of=8\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ERi9V3r6u_NM"
      },
      "outputs": [],
      "source": [
        "args = TrainingArguments(\n",
        "    output_dir=\"gpt2-chords\",\n",
        "    c=32,\n",
        "    per_device_eval_batch_size=32,\n",
        "    gradient_accumulation_steps=2,\n",
        "    num_train_epochs=3,\n",
        "    learning_rate=5e-5,\n",
        "    weight_decay=0.01,\n",
        "    warmup_ratio=0.1,\n",
        "    fp16=True,\n",
        "    logging_steps=100,\n",
        "    save_strategy=\"epoch\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    gradient_checkpointing=True,\n",
        "    max_grad_norm=1.0,\n",
        "    report_to=\"none\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mjVm1wCKvCRw"
      },
      "outputs": [],
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=args,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=val_ds,\n",
        "    data_collator=collator\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8f5EsuBVesj1",
        "outputId": "35c87b01-6413-4ab8-f4b7-ccaf618e9b52"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
            "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
          ]
        },
        {
          "data": {
            "text/html": "\n    <div>\n      \n      <progress value='25395' max='25395' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [25395/25395 7:40:35, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>100</td>\n      <td>2.914000</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>2.210300</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>1.758400</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>1.483400</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>1.337000</td>\n    </tr>\n    <tr>\n      <td>600</td>\n      <td>1.264600</td>\n    </tr>\n    <tr>\n      <td>700</td>\n      <td>1.224400</td>\n    </tr>\n    <tr>\n      <td>800</td>\n      <td>1.197300</td>\n    </tr>\n    <tr>\n      <td>900</td>\n      <td>1.160600</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>1.132900</td>\n    </tr>\n    <tr>\n      <td>1100</td>\n      <td>1.136600</td>\n    </tr>\n    <tr>\n      <td>1200</td>\n      <td>1.115800</td>\n    </tr>\n    <tr>\n      <td>1300</td>\n      <td>1.096200</td>\n    </tr>\n    <tr>\n      <td>1400</td>\n      <td>1.094000</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>1.080900</td>\n    </tr>\n    <tr>\n      <td>1600</td>\n      <td>1.069800</td>\n    </tr>\n    <tr>\n      <td>1700</td>\n      <td>1.056300</td>\n    </tr>\n    <tr>\n      <td>1800</td>\n      <td>1.058500</td>\n    </tr>\n    <tr>\n      <td>1900</td>\n      <td>1.051500</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>1.038300</td>\n    </tr>\n    <tr>\n      <td>2100</td>\n      <td>1.027900</td>\n    </tr>\n    <tr>\n      <td>2200</td>\n      <td>1.010500</td>\n    </tr>\n    <tr>\n      <td>2300</td>\n      <td>1.009100</td>\n    </tr>\n    <tr>\n      <td>2400</td>\n      <td>1.038100</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>0.999700</td>\n    </tr>\n    <tr>\n      <td>2600</td>\n      <td>1.010100</td>\n    </tr>\n    <tr>\n      <td>2700</td>\n      <td>0.990800</td>\n    </tr>\n    <tr>\n      <td>2800</td>\n      <td>0.998900</td>\n    </tr>\n    <tr>\n      <td>2900</td>\n      <td>0.986000</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>0.992100</td>\n    </tr>\n    <tr>\n      <td>3100</td>\n      <td>0.969400</td>\n    </tr>\n    <tr>\n      <td>3200</td>\n      <td>0.960500</td>\n    </tr>\n    <tr>\n      <td>3300</td>\n      <td>0.990300</td>\n    </tr>\n    <tr>\n      <td>3400</td>\n      <td>0.966900</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>0.963300</td>\n    </tr>\n    <tr>\n      <td>3600</td>\n      <td>0.969100</td>\n    </tr>\n    <tr>\n      <td>3700</td>\n      <td>0.948500</td>\n    </tr>\n    <tr>\n      <td>3800</td>\n      <td>0.959700</td>\n    </tr>\n    <tr>\n      <td>3900</td>\n      <td>0.984600</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>0.961700</td>\n    </tr>\n    <tr>\n      <td>4100</td>\n      <td>0.950500</td>\n    </tr>\n    <tr>\n      <td>4200</td>\n      <td>0.938800</td>\n    </tr>\n    <tr>\n      <td>4300</td>\n      <td>0.949400</td>\n    </tr>\n    <tr>\n      <td>4400</td>\n      <td>0.940300</td>\n    </tr>\n    <tr>\n      <td>4500</td>\n      <td>0.960900</td>\n    </tr>\n    <tr>\n      <td>4600</td>\n      <td>0.946800</td>\n    </tr>\n    <tr>\n      <td>4700</td>\n      <td>0.954000</td>\n    </tr>\n    <tr>\n      <td>4800</td>\n      <td>0.945400</td>\n    </tr>\n    <tr>\n      <td>4900</td>\n      <td>0.944400</td>\n    </tr>\n    <tr>\n      <td>5000</td>\n      <td>0.941700</td>\n    </tr>\n    <tr>\n      <td>5100</td>\n      <td>0.936000</td>\n    </tr>\n    <tr>\n      <td>5200</td>\n      <td>0.941600</td>\n    </tr>\n    <tr>\n      <td>5300</td>\n      <td>0.941300</td>\n    </tr>\n    <tr>\n      <td>5400</td>\n      <td>0.928400</td>\n    </tr>\n    <tr>\n      <td>5500</td>\n      <td>0.928600</td>\n    </tr>\n    <tr>\n      <td>5600</td>\n      <td>0.929300</td>\n    </tr>\n    <tr>\n      <td>5700</td>\n      <td>0.945500</td>\n    </tr>\n    <tr>\n      <td>5800</td>\n      <td>0.915400</td>\n    </tr>\n    <tr>\n      <td>5900</td>\n      <td>0.926000</td>\n    </tr>\n    <tr>\n      <td>6000</td>\n      <td>0.935200</td>\n    </tr>\n    <tr>\n      <td>6100</td>\n      <td>0.941000</td>\n    </tr>\n    <tr>\n      <td>6200</td>\n      <td>0.927400</td>\n    </tr>\n    <tr>\n      <td>6300</td>\n      <td>0.942000</td>\n    </tr>\n    <tr>\n      <td>6400</td>\n      <td>0.920400</td>\n    </tr>\n    <tr>\n      <td>6500</td>\n      <td>0.919600</td>\n    </tr>\n    <tr>\n      <td>6600</td>\n      <td>0.931200</td>\n    </tr>\n    <tr>\n      <td>6700</td>\n      <td>0.918900</td>\n    </tr>\n    <tr>\n      <td>6800</td>\n      <td>0.923200</td>\n    </tr>\n    <tr>\n      <td>6900</td>\n      <td>0.927900</td>\n    </tr>\n    <tr>\n      <td>7000</td>\n      <td>0.927000</td>\n    </tr>\n    <tr>\n      <td>7100</td>\n      <td>0.924200</td>\n    </tr>\n    <tr>\n      <td>7200</td>\n      <td>0.924500</td>\n    </tr>\n    <tr>\n      <td>7300</td>\n      <td>0.915500</td>\n    </tr>\n    <tr>\n      <td>7400</td>\n      <td>0.908800</td>\n    </tr>\n    <tr>\n      <td>7500</td>\n      <td>0.911000</td>\n    </tr>\n    <tr>\n      <td>7600</td>\n      <td>0.905400</td>\n    </tr>\n    <tr>\n      <td>7700</td>\n      <td>0.915100</td>\n    </tr>\n    <tr>\n      <td>7800</td>\n      <td>0.906000</td>\n    </tr>\n    <tr>\n      <td>7900</td>\n      <td>0.912500</td>\n    </tr>\n    <tr>\n      <td>8000</td>\n      <td>0.917400</td>\n    </tr>\n    <tr>\n      <td>8100</td>\n      <td>0.918100</td>\n    </tr>\n    <tr>\n      <td>8200</td>\n      <td>0.903400</td>\n    </tr>\n    <tr>\n      <td>8300</td>\n      <td>0.907500</td>\n    </tr>\n    <tr>\n      <td>8400</td>\n      <td>0.915600</td>\n    </tr>\n    <tr>\n      <td>8500</td>\n      <td>0.908700</td>\n    </tr>\n    <tr>\n      <td>8600</td>\n      <td>0.905800</td>\n    </tr>\n    <tr>\n      <td>8700</td>\n      <td>0.890600</td>\n    </tr>\n    <tr>\n      <td>8800</td>\n      <td>0.907000</td>\n    </tr>\n    <tr>\n      <td>8900</td>\n      <td>0.910900</td>\n    </tr>\n    <tr>\n      <td>9000</td>\n      <td>0.900400</td>\n    </tr>\n    <tr>\n      <td>9100</td>\n      <td>0.903600</td>\n    </tr>\n    <tr>\n      <td>9200</td>\n      <td>0.901300</td>\n    </tr>\n    <tr>\n      <td>9300</td>\n      <td>0.895100</td>\n    </tr>\n    <tr>\n      <td>9400</td>\n      <td>0.894500</td>\n    </tr>\n    <tr>\n      <td>9500</td>\n      <td>0.906700</td>\n    </tr>\n    <tr>\n      <td>9600</td>\n      <td>0.898500</td>\n    </tr>\n    <tr>\n      <td>9700</td>\n      <td>0.907100</td>\n    </tr>\n    <tr>\n      <td>9800</td>\n      <td>0.901500</td>\n    </tr>\n    <tr>\n      <td>9900</td>\n      <td>0.887200</td>\n    </tr>\n    <tr>\n      <td>10000</td>\n      <td>0.912800</td>\n    </tr>\n    <tr>\n      <td>10100</td>\n      <td>0.902100</td>\n    </tr>\n    <tr>\n      <td>10200</td>\n      <td>0.897900</td>\n    </tr>\n    <tr>\n      <td>10300</td>\n      <td>0.892100</td>\n    </tr>\n    <tr>\n      <td>10400</td>\n      <td>0.904500</td>\n    </tr>\n    <tr>\n      <td>10500</td>\n      <td>0.910300</td>\n    </tr>\n    <tr>\n      <td>10600</td>\n      <td>0.894200</td>\n    </tr>\n    <tr>\n      <td>10700</td>\n      <td>0.900700</td>\n    </tr>\n    <tr>\n      <td>10800</td>\n      <td>0.890300</td>\n    </tr>\n    <tr>\n      <td>10900</td>\n      <td>0.895200</td>\n    </tr>\n    <tr>\n      <td>11000</td>\n      <td>0.899600</td>\n    </tr>\n    <tr>\n      <td>11100</td>\n      <td>0.898800</td>\n    </tr>\n    <tr>\n      <td>11200</td>\n      <td>0.899700</td>\n    </tr>\n    <tr>\n      <td>11300</td>\n      <td>0.895300</td>\n    </tr>\n    <tr>\n      <td>11400</td>\n      <td>0.886600</td>\n    </tr>\n    <tr>\n      <td>11500</td>\n      <td>0.889200</td>\n    </tr>\n    <tr>\n      <td>11600</td>\n      <td>0.894000</td>\n    </tr>\n    <tr>\n      <td>11700</td>\n      <td>0.885200</td>\n    </tr>\n    <tr>\n      <td>11800</td>\n      <td>0.893700</td>\n    </tr>\n    <tr>\n      <td>11900</td>\n      <td>0.883100</td>\n    </tr>\n    <tr>\n      <td>12000</td>\n      <td>0.886000</td>\n    </tr>\n    <tr>\n      <td>12100</td>\n      <td>0.892500</td>\n    </tr>\n    <tr>\n      <td>12200</td>\n      <td>0.883700</td>\n    </tr>\n    <tr>\n      <td>12300</td>\n      <td>0.892700</td>\n    </tr>\n    <tr>\n      <td>12400</td>\n      <td>0.891100</td>\n    </tr>\n    <tr>\n      <td>12500</td>\n      <td>0.900900</td>\n    </tr>\n    <tr>\n      <td>12600</td>\n      <td>0.892500</td>\n    </tr>\n    <tr>\n      <td>12700</td>\n      <td>0.890900</td>\n    </tr>\n    <tr>\n      <td>12800</td>\n      <td>0.885200</td>\n    </tr>\n    <tr>\n      <td>12900</td>\n      <td>0.896000</td>\n    </tr>\n    <tr>\n      <td>13000</td>\n      <td>0.884900</td>\n    </tr>\n    <tr>\n      <td>13100</td>\n      <td>0.902700</td>\n    </tr>\n    <tr>\n      <td>13200</td>\n      <td>0.889900</td>\n    </tr>\n    <tr>\n      <td>13300</td>\n      <td>0.877500</td>\n    </tr>\n    <tr>\n      <td>13400</td>\n      <td>0.876100</td>\n    </tr>\n    <tr>\n      <td>13500</td>\n      <td>0.872500</td>\n    </tr>\n    <tr>\n      <td>13600</td>\n      <td>0.898500</td>\n    </tr>\n    <tr>\n      <td>13700</td>\n      <td>0.880500</td>\n    </tr>\n    <tr>\n      <td>13800</td>\n      <td>0.883000</td>\n    </tr>\n    <tr>\n      <td>13900</td>\n      <td>0.895100</td>\n    </tr>\n    <tr>\n      <td>14000</td>\n      <td>0.881700</td>\n    </tr>\n    <tr>\n      <td>14100</td>\n      <td>0.871400</td>\n    </tr>\n    <tr>\n      <td>14200</td>\n      <td>0.895200</td>\n    </tr>\n    <tr>\n      <td>14300</td>\n      <td>0.891800</td>\n    </tr>\n    <tr>\n      <td>14400</td>\n      <td>0.889100</td>\n    </tr>\n    <tr>\n      <td>14500</td>\n      <td>0.879700</td>\n    </tr>\n    <tr>\n      <td>14600</td>\n      <td>0.885200</td>\n    </tr>\n    <tr>\n      <td>14700</td>\n      <td>0.885100</td>\n    </tr>\n    <tr>\n      <td>14800</td>\n      <td>0.880000</td>\n    </tr>\n    <tr>\n      <td>14900</td>\n      <td>0.893100</td>\n    </tr>\n    <tr>\n      <td>15000</td>\n      <td>0.889800</td>\n    </tr>\n    <tr>\n      <td>15100</td>\n      <td>0.882200</td>\n    </tr>\n    <tr>\n      <td>15200</td>\n      <td>0.886000</td>\n    </tr>\n    <tr>\n      <td>15300</td>\n      <td>0.880800</td>\n    </tr>\n    <tr>\n      <td>15400</td>\n      <td>0.880600</td>\n    </tr>\n    <tr>\n      <td>15500</td>\n      <td>0.884900</td>\n    </tr>\n    <tr>\n      <td>15600</td>\n      <td>0.889800</td>\n    </tr>\n    <tr>\n      <td>15700</td>\n      <td>0.879300</td>\n    </tr>\n    <tr>\n      <td>15800</td>\n      <td>0.893600</td>\n    </tr>\n    <tr>\n      <td>15900</td>\n      <td>0.888000</td>\n    </tr>\n    <tr>\n      <td>16000</td>\n      <td>0.882800</td>\n    </tr>\n    <tr>\n      <td>16100</td>\n      <td>0.883600</td>\n    </tr>\n    <tr>\n      <td>16200</td>\n      <td>0.875000</td>\n    </tr>\n    <tr>\n      <td>16300</td>\n      <td>0.886400</td>\n    </tr>\n    <tr>\n      <td>16400</td>\n      <td>0.870600</td>\n    </tr>\n    <tr>\n      <td>16500</td>\n      <td>0.885200</td>\n    </tr>\n    <tr>\n      <td>16600</td>\n      <td>0.871900</td>\n    </tr>\n    <tr>\n      <td>16700</td>\n      <td>0.889700</td>\n    </tr>\n    <tr>\n      <td>16800</td>\n      <td>0.879300</td>\n    </tr>\n    <tr>\n      <td>16900</td>\n      <td>0.879000</td>\n    </tr>\n    <tr>\n      <td>17000</td>\n      <td>0.866600</td>\n    </tr>\n    <tr>\n      <td>17100</td>\n      <td>0.876700</td>\n    </tr>\n    <tr>\n      <td>17200</td>\n      <td>0.865700</td>\n    </tr>\n    <tr>\n      <td>17300</td>\n      <td>0.876400</td>\n    </tr>\n    <tr>\n      <td>17400</td>\n      <td>0.871400</td>\n    </tr>\n    <tr>\n      <td>17500</td>\n      <td>0.877500</td>\n    </tr>\n    <tr>\n      <td>17600</td>\n      <td>0.880000</td>\n    </tr>\n    <tr>\n      <td>17700</td>\n      <td>0.875200</td>\n    </tr>\n    <tr>\n      <td>17800</td>\n      <td>0.886500</td>\n    </tr>\n    <tr>\n      <td>17900</td>\n      <td>0.874800</td>\n    </tr>\n    <tr>\n      <td>18000</td>\n      <td>0.885800</td>\n    </tr>\n    <tr>\n      <td>18100</td>\n      <td>0.873100</td>\n    </tr>\n    <tr>\n      <td>18200</td>\n      <td>0.870000</td>\n    </tr>\n    <tr>\n      <td>18300</td>\n      <td>0.876900</td>\n    </tr>\n    <tr>\n      <td>18400</td>\n      <td>0.861200</td>\n    </tr>\n    <tr>\n      <td>18500</td>\n      <td>0.882000</td>\n    </tr>\n    <tr>\n      <td>18600</td>\n      <td>0.868700</td>\n    </tr>\n    <tr>\n      <td>18700</td>\n      <td>0.866100</td>\n    </tr>\n    <tr>\n      <td>18800</td>\n      <td>0.871300</td>\n    </tr>\n    <tr>\n      <td>18900</td>\n      <td>0.876100</td>\n    </tr>\n    <tr>\n      <td>19000</td>\n      <td>0.870300</td>\n    </tr>\n    <tr>\n      <td>19100</td>\n      <td>0.869600</td>\n    </tr>\n    <tr>\n      <td>19200</td>\n      <td>0.865100</td>\n    </tr>\n    <tr>\n      <td>19300</td>\n      <td>0.875900</td>\n    </tr>\n    <tr>\n      <td>19400</td>\n      <td>0.877000</td>\n    </tr>\n    <tr>\n      <td>19500</td>\n      <td>0.879000</td>\n    </tr>\n    <tr>\n      <td>19600</td>\n      <td>0.872800</td>\n    </tr>\n    <tr>\n      <td>19700</td>\n      <td>0.862500</td>\n    </tr>\n    <tr>\n      <td>19800</td>\n      <td>0.871600</td>\n    </tr>\n    <tr>\n      <td>19900</td>\n      <td>0.877700</td>\n    </tr>\n    <tr>\n      <td>20000</td>\n      <td>0.871700</td>\n    </tr>\n    <tr>\n      <td>20100</td>\n      <td>0.867800</td>\n    </tr>\n    <tr>\n      <td>20200</td>\n      <td>0.873300</td>\n    </tr>\n    <tr>\n      <td>20300</td>\n      <td>0.879500</td>\n    </tr>\n    <tr>\n      <td>20400</td>\n      <td>0.881600</td>\n    </tr>\n    <tr>\n      <td>20500</td>\n      <td>0.871400</td>\n    </tr>\n    <tr>\n      <td>20600</td>\n      <td>0.869000</td>\n    </tr>\n    <tr>\n      <td>20700</td>\n      <td>0.869900</td>\n    </tr>\n    <tr>\n      <td>20800</td>\n      <td>0.883000</td>\n    </tr>\n    <tr>\n      <td>20900</td>\n      <td>0.859600</td>\n    </tr>\n    <tr>\n      <td>21000</td>\n      <td>0.874500</td>\n    </tr>\n    <tr>\n      <td>21100</td>\n      <td>0.876600</td>\n    </tr>\n    <tr>\n      <td>21200</td>\n      <td>0.876600</td>\n    </tr>\n    <tr>\n      <td>21300</td>\n      <td>0.871800</td>\n    </tr>\n    <tr>\n      <td>21400</td>\n      <td>0.871300</td>\n    </tr>\n    <tr>\n      <td>21500</td>\n      <td>0.867900</td>\n    </tr>\n    <tr>\n      <td>21600</td>\n      <td>0.873600</td>\n    </tr>\n    <tr>\n      <td>21700</td>\n      <td>0.870400</td>\n    </tr>\n    <tr>\n      <td>21800</td>\n      <td>0.871300</td>\n    </tr>\n    <tr>\n      <td>21900</td>\n      <td>0.884000</td>\n    </tr>\n    <tr>\n      <td>22000</td>\n      <td>0.871300</td>\n    </tr>\n    <tr>\n      <td>22100</td>\n      <td>0.868700</td>\n    </tr>\n    <tr>\n      <td>22200</td>\n      <td>0.877200</td>\n    </tr>\n    <tr>\n      <td>22300</td>\n      <td>0.877900</td>\n    </tr>\n    <tr>\n      <td>22400</td>\n      <td>0.861900</td>\n    </tr>\n    <tr>\n      <td>22500</td>\n      <td>0.871800</td>\n    </tr>\n    <tr>\n      <td>22600</td>\n      <td>0.893300</td>\n    </tr>\n    <tr>\n      <td>22700</td>\n      <td>0.860500</td>\n    </tr>\n    <tr>\n      <td>22800</td>\n      <td>0.876300</td>\n    </tr>\n    <tr>\n      <td>22900</td>\n      <td>0.880100</td>\n    </tr>\n    <tr>\n      <td>23000</td>\n      <td>0.865700</td>\n    </tr>\n    <tr>\n      <td>23100</td>\n      <td>0.866000</td>\n    </tr>\n    <tr>\n      <td>23200</td>\n      <td>0.882800</td>\n    </tr>\n    <tr>\n      <td>23300</td>\n      <td>0.868300</td>\n    </tr>\n    <tr>\n      <td>23400</td>\n      <td>0.876700</td>\n    </tr>\n    <tr>\n      <td>23500</td>\n      <td>0.867400</td>\n    </tr>\n    <tr>\n      <td>23600</td>\n      <td>0.877100</td>\n    </tr>\n    <tr>\n      <td>23700</td>\n      <td>0.871600</td>\n    </tr>\n    <tr>\n      <td>23800</td>\n      <td>0.860000</td>\n    </tr>\n    <tr>\n      <td>23900</td>\n      <td>0.876000</td>\n    </tr>\n    <tr>\n      <td>24000</td>\n      <td>0.867700</td>\n    </tr>\n    <tr>\n      <td>24100</td>\n      <td>0.867000</td>\n    </tr>\n    <tr>\n      <td>24200</td>\n      <td>0.882300</td>\n    </tr>\n    <tr>\n      <td>24300</td>\n      <td>0.876700</td>\n    </tr>\n    <tr>\n      <td>24400</td>\n      <td>0.859000</td>\n    </tr>\n    <tr>\n      <td>24500</td>\n      <td>0.862500</td>\n    </tr>\n    <tr>\n      <td>24600</td>\n      <td>0.868500</td>\n    </tr>\n    <tr>\n      <td>24700</td>\n      <td>0.868500</td>\n    </tr>\n    <tr>\n      <td>24800</td>\n      <td>0.872800</td>\n    </tr>\n    <tr>\n      <td>24900</td>\n      <td>0.871200</td>\n    </tr>\n    <tr>\n      <td>25000</td>\n      <td>0.872000</td>\n    </tr>\n    <tr>\n      <td>25100</td>\n      <td>0.857700</td>\n    </tr>\n    <tr>\n      <td>25200</td>\n      <td>0.866500</td>\n    </tr>\n    <tr>\n      <td>25300</td>\n      <td>0.873500</td>\n    </tr>\n  </tbody>\n</table><p>",
            "text/plain": "<IPython.core.display.HTML object>"
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Model & tokenizer saved to gpt2-chords-final\n"
          ]
        }
      ],
      "source": [
        "trainer.train()\n",
        "trainer.save_model(\"gpt2-chords-final\")\n",
        "tokenizer.save_pretrained(\"gpt2-chords-final\")\n",
        "print(\"✅ Model & tokenizer saved to gpt2-chords-final\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yxdWn1G-XKIs"
      },
      "source": [
        "# Evaluation & Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cyi7cuGeSk-R",
        "outputId": "5eff478a-a5bf-41eb-f19d-685c602bd15c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XciISphEF8ta",
        "outputId": "984a3154-4e04-46d9-e981-3993e99c9cff"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "W0507 09:34:26.596000 184 torch/_inductor/utils.py:1137] [0/3] Not enough SMs to use max_autotune_gemm mode\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PPL=2.393 | top1=76.62% | top5=94.94% | top8=96.83%\n"
          ]
        }
      ],
      "source": [
        "import torch, math, torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "loader = DataLoader(\n",
        "    test_ds,\n",
        "    batch_size=64,\n",
        "    collate_fn=collator,\n",
        "    shuffle=False,\n",
        "    num_workers=8,\n",
        "    pin_memory=True,\n",
        "    persistent_workers=True,\n",
        ")\n",
        "\n",
        "model_dir = \"gpt2-chords-final\"\n",
        "tokenizer  = PreTrainedTokenizerFast.from_pretrained(model_dir)\n",
        "model      = AutoModelForCausalLM.from_pretrained(model_dir)\n",
        "collator   = DataCollatorForLanguageModeling(\n",
        "                 tokenizer=tokenizer, mlm=False, pad_to_multiple_of=8)\n",
        "\n",
        "device = torch.device(\"cuda\")\n",
        "model = model.half().to(device).eval()\n",
        "torch.backends.cudnn.benchmark = True\n",
        "try:\n",
        "    model = torch.compile(model, mode=\"reduce-overhead\")\n",
        "except: pass\n",
        "\n",
        "total_nll = 0.0\n",
        "total_tok = 0\n",
        "top1_hit  = 0\n",
        "top5_hit  = 0\n",
        "top8_hit  = 0\n",
        "pad_id    = tokenizer.pad_token_id\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in loader:\n",
        "        inp = batch[\"input_ids\"].to(device, non_blocking=True)\n",
        "        out = model(inp)\n",
        "        logits = out.logits[:, :-1, :]\n",
        "        labels = inp[:, 1:]\n",
        "        mask   = labels != pad_id\n",
        "\n",
        "        # NLL\n",
        "        loss = F.cross_entropy(\n",
        "            logits.reshape(-1, logits.size(-1)),\n",
        "            labels.reshape(-1),\n",
        "            ignore_index=pad_id,\n",
        "            reduction=\"sum\"\n",
        "        )\n",
        "        total_nll += loss.item()\n",
        "        total_tok += mask.sum().item()\n",
        "\n",
        "        # Top-k\n",
        "        topks = logits.topk(8, dim=-1).indices  # [B, L, 8]\n",
        "        corr  = (topks == labels.unsqueeze(-1)) & mask.unsqueeze(-1)\n",
        "        top1_hit += corr[..., :1].any(-1).sum().item()\n",
        "        top5_hit += corr[..., :5].any(-1).sum().item()\n",
        "        top8_hit += corr.any(-1).sum().item()\n",
        "\n",
        "# สรุป\n",
        "ppl  = math.exp(total_nll / total_tok)\n",
        "acc1 = top1_hit / total_tok\n",
        "acc5 = top5_hit / total_tok\n",
        "acc8 = top8_hit / total_tok\n",
        "\n",
        "print(f\"PPL={ppl:.3f} | top1={acc1:.2%} | top5={acc5:.2%} | top8={acc8:.2%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Circle of fifts"
      ],
      "metadata": {
        "id": "WmumP_m_h0Q8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re, math, numpy as np, torch, music21 as m21\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import (AutoModelForCausalLM,\n",
        "                          PreTrainedTokenizerFast,\n",
        "                          DataCollatorForLanguageModeling)\n",
        "\n",
        "ROOT_RE = re.compile(r\"^([A-Ga-g])([#b♯♭]?)(.*)$\")\n",
        "PC_NUM  = {\"C\":0,\"C#\":1,\"Db\":1,\"D\":2,\"D#\":3,\"Eb\":3,\"E\":4,\"F\":5,\"F#\":6,\"Gb\":6,\n",
        "           \"G\":7,\"G#\":8,\"Ab\":8,\"A\":9,\"A#\":10,\"Bb\":10,\"B\":11,\"Cb\":11}\n",
        "\n",
        "def parse_root(tok:str)->int|None:\n",
        "    \"\"\"ดึง root-note → 0-11  (None ถ้าไม่รู้จัก)\"\"\"\n",
        "    m = ROOT_RE.match(tok.strip())\n",
        "    if not m: return None\n",
        "    note = m.group(1).upper() + m.group(2).replace('♯','#').replace('♭','b')\n",
        "    return PC_NUM.get(note)\n",
        "\n",
        "def circ_dist(a:int,b:int)->int:\n",
        "    \"\"\"ระยะสั้นสุดบนวง 12-tone (0-6 semitones)\"\"\"\n",
        "    diff = abs(a-b)\n",
        "    return min(diff, 12-diff)\n",
        "\n",
        "class Seed4Dataset(torch.utils.data.Dataset):\n",
        "    \"\"\"เก็บ (seed_ids[4], target_ids[32]) ต่อเพลง\"\"\"\n",
        "    def __init__(self, base_ds, seed_len=4, gen_len=32):\n",
        "        self.samples = []\n",
        "        for d in base_ds:\n",
        "            ids = d[\"input_ids\"]\n",
        "            if len(ids) >= seed_len + gen_len:\n",
        "                self.samples.append((ids[:seed_len], ids[seed_len:seed_len+gen_len]))\n",
        "    def __len__(self):              return len(self.samples)\n",
        "    def __getitem__(self, idx):     return self.samples[idx]\n",
        "\n",
        "model_dir = \"drive/MyDrive/project-main/gpt2-chords-final\"\n",
        "tok = PreTrainedTokenizerFast.from_pretrained(model_dir)\n",
        "tok.padding_side = \"left\"\n",
        "tok.pad_token = \"<pad>\"\n",
        "model     = AutoModelForCausalLM.from_pretrained(model_dir)\n",
        "\n",
        "old_wpe = model.transformer.wpe          # [128, 768]\n",
        "new_len = 512\n",
        "new_wpe = torch.nn.Embedding(new_len, old_wpe.embedding_dim)\n",
        "new_wpe.weight.data[:old_wpe.num_embeddings] = old_wpe.weight.data.clone()\n",
        "model.transformer.wpe = new_wpe\n",
        "model.config.n_positions = new_len\n",
        "model.config.n_ctx       = new_len\n",
        "model.config.max_position_embeddings = new_len\n",
        "\n",
        "collator  = DataCollatorForLanguageModeling(\n",
        "               tokenizer=tok, mlm=False, pad_to_multiple_of=8)\n",
        "\n",
        "device = torch.device(\"cuda\")\n",
        "model.to(device).eval()\n",
        "\n",
        "seed_ds = Seed4Dataset(test_ds, 4, 32)\n",
        "def collate(batch):\n",
        "    seeds   = [torch.tensor(s[0]) for s in batch]\n",
        "    targets = [torch.tensor(s[1]) for s in batch]\n",
        "    seeds   = torch.nn.utils.rnn.pad_sequence(\n",
        "                  seeds, batch_first=True, padding_value=tok.pad_token_id)\n",
        "    return {\"seed\": seeds, \"gold\": targets}\n",
        "\n",
        "loader = DataLoader(seed_ds, batch_size=64, collate_fn=collate,\n",
        "                    shuffle=False, num_workers=4, pin_memory=True)\n",
        "\n",
        "pred_step_sum = gold_step_sum = 0\n",
        "pred_step_cnt = gold_step_cnt = 0\n",
        "\n",
        "with torch.inference_mode():\n",
        "    for batch in loader:\n",
        "        seed_ids = batch[\"seed\"].to(\"cuda\")\n",
        "        gold_ids = batch[\"gold\"]\n",
        "\n",
        "        gen_ids =model.generate(seed_ids,max_new_tokens=32,do_sample=True,\n",
        "                           top_p=0.9,temperature=1.1,\n",
        "                           no_repeat_ngram_size=3,repetition_penalty=1.1,\n",
        "                           pad_token_id=tok.pad_token_id,\n",
        "                           eos_token_id=tok.eos_token_id)\n",
        "\n",
        "        for gen, gold in zip(gen_ids.cpu(), gold_ids):\n",
        "            pred_tokens = [t for t in\n",
        "                           tok.convert_ids_to_tokens(gen.tolist(),\n",
        "                                                     skip_special_tokens=True)\n",
        "                           if t not in (\"<bos>\",\"<pad>\",\"<eos>\")][-32:]\n",
        "            gold_tokens = tok.convert_ids_to_tokens(gold.tolist())\n",
        "\n",
        "            for a, b in zip(gold_tokens[:-1], gold_tokens[1:]):\n",
        "                ra, rb = parse_root(a), parse_root(b)\n",
        "                if ra is not None and rb is not None:\n",
        "                    gold_step_sum += circ_dist(ra, rb)\n",
        "                    gold_step_cnt += 1\n",
        "\n",
        "            for a, b in zip(pred_tokens[:-1], pred_tokens[1:]):\n",
        "                ra, rb = parse_root(a), parse_root(b)\n",
        "                if ra is not None and rb is not None:\n",
        "                    pred_step_sum += circ_dist(ra, rb)\n",
        "                    pred_step_cnt += 1\n",
        "\n",
        "avg_gold = gold_step_sum / gold_step_cnt\n",
        "avg_pred = pred_step_sum / pred_step_cnt\n",
        "\n",
        "print(f\"‣ Gold 32-chord step distance  = {avg_gold:.3f} semitones\")\n",
        "print(f\"‣ Pred 32-chord step distance  = {avg_pred:.3f} semitones\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gWVYXtR4hy3L",
        "outputId": "37a0e531-f000-4aa4-af17-9866782512f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "<ipython-input-10-12aca3ee0a3d>:74: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  seeds   = [torch.tensor(s[0]) for s in batch]\n",
            "<ipython-input-10-12aca3ee0a3d>:74: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  seeds   = [torch.tensor(s[0]) for s in batch]\n",
            "<ipython-input-10-12aca3ee0a3d>:74: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  seeds   = [torch.tensor(s[0]) for s in batch]\n",
            "<ipython-input-10-12aca3ee0a3d>:75: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  targets = [torch.tensor(s[1]) for s in batch]\n",
            "<ipython-input-10-12aca3ee0a3d>:75: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  targets = [torch.tensor(s[1]) for s in batch]\n",
            "<ipython-input-10-12aca3ee0a3d>:74: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  seeds   = [torch.tensor(s[0]) for s in batch]\n",
            "<ipython-input-10-12aca3ee0a3d>:75: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  targets = [torch.tensor(s[1]) for s in batch]\n",
            "<ipython-input-10-12aca3ee0a3d>:75: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  targets = [torch.tensor(s[1]) for s in batch]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‣ Gold 32-chord step distance  = 3.551 semitones\n",
            "‣ Pred 32-chord step distance  = 3.175 semitones\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CD, CM, TS"
      ],
      "metadata": {
        "id": "uzhVUvq9VL3u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import re, math, numpy as np, torch, music21 as m21\n",
        "from functools import lru_cache\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import (AutoModelForCausalLM,\n",
        "                          PreTrainedTokenizerFast,\n",
        "                          DataCollatorForLanguageModeling)\n",
        "\n",
        "ROOT_RE = re.compile(r\"^([A-Ga-g])([#b♯♭]?)(.*)$\")\n",
        "PC_NUM  = {'C':0,'C#':1,'Db':1,'D':2,'D#':3,'Eb':3,'E':4,'F':5,'F#':6,'Gb':6,\n",
        "           'G':7,'G#':8,'Ab':8,'A':9,'A#':10,'Bb':10,'B':11,'Cb':11}\n",
        "\n",
        "def circ_dist(a:int,b:int)->int:      # 0-6\n",
        "    d=abs(a-b); return min(d,12-d)\n",
        "\n",
        "@lru_cache(maxsize=4096)\n",
        "def chord_vec(token:str)->np.ndarray:\n",
        "    pcs=set()\n",
        "    try:\n",
        "        cs=m21.harmony.ChordSymbol(token)\n",
        "        pcs={p.pitchClass for p in cs.pitches}\n",
        "    except Exception:\n",
        "        m=ROOT_RE.match(token)\n",
        "        if m:\n",
        "            root=PC_NUM[m.group(1).upper()+m.group(2).replace('♯','#').replace('♭','b')]\n",
        "            pcs={root,(root+7)%12}\n",
        "            pcs.add((root+3)%12 if 'm' in token.lower() else (root+4)%12)\n",
        "        else:\n",
        "            pcs={0}\n",
        "    v=np.zeros(12); v[list(pcs)]=1\n",
        "    return v/len(pcs)\n",
        "\n",
        "def cloud_diam(pcs:set[int])->int:\n",
        "    if len(pcs)<2: return 0\n",
        "    return max(circ_dist(a,b) for a in pcs for b in pcs)\n",
        "\n",
        "def quick_key(chords:list[str])->np.ndarray:\n",
        "    \"\"\"root-histogram → tonic triad vector\"\"\"\n",
        "    roots=[c[0].upper() for c in chords if c]\n",
        "    root=max(set(roots),key=roots.count) if roots else 'C'\n",
        "    mode='minor' if any(c.lower().endswith(('m','min')) for c in chords) else 'major'\n",
        "    return chord_vec(f\"{root}{'m' if mode=='minor' else ''}\")\n",
        "\n",
        "class Seed4(torch.utils.data.Dataset):\n",
        "    def __init__(self, base, seed=4, gen=32):\n",
        "        self.items=[(d[\"input_ids\"][:seed],d[\"input_ids\"][seed:seed+gen])\n",
        "                    for d in base if len(d[\"input_ids\"])>=seed+gen]\n",
        "    def __len__(self):  return len(self.items)\n",
        "    def __getitem__(self,i):\n",
        "        s,g=self.items[i];return{\"seed\":torch.tensor(s),\"gold\":torch.tensor(g)}\n",
        "\n",
        "def collate(batch):\n",
        "    seeds=[b[\"seed\"] for b in batch]\n",
        "    golds=[b[\"gold\"] for b in batch]\n",
        "    seeds=torch.nn.utils.rnn.pad_sequence(\n",
        "        seeds,batch_first=True,padding_value=tok.pad_token_id)\n",
        "    return{\"seed\":seeds,\"gold\":golds}\n",
        "\n",
        "\n",
        "model_dir=\"drive/MyDrive/project-main/gpt2-chords-final\"\n",
        "tok = PreTrainedTokenizerFast.from_pretrained(model_dir)\n",
        "tok.padding_side, tok.pad_token=\"left\",\"<pad>\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_dir).half().to(\"cuda\").eval()\n",
        "\n",
        "seed_ds  = Seed4(test_ds,4,32)\n",
        "loader   = DataLoader(seed_ds,batch_size=64,collate_fn=collate,\n",
        "                      shuffle=False,num_workers=4,pin_memory=True)\n",
        "\n",
        "def bucket(): return {\"cd\":0,\"cm\":0,\"ts\":0,\"n_cd\":0,\"n_cm\":0,\"n_ts\":0}\n",
        "gold, pred = bucket(), bucket()\n",
        "\n",
        "def update(tokens,acc,tonic_vec):\n",
        "    prev=None\n",
        "    for t in tokens:\n",
        "        v=chord_vec(t); pcs=np.where(v>0)[0]\n",
        "        acc[\"cd\"]+=cloud_diam(pcs); acc[\"n_cd\"]+=1\n",
        "        acc[\"ts\"]+=np.linalg.norm(v-tonic_vec); acc[\"n_ts\"]+=1\n",
        "        if prev is not None:\n",
        "            acc[\"cm\"]+=np.linalg.norm(v-prev); acc[\"n_cm\"]+=1\n",
        "        prev=v\n",
        "\n",
        "with torch.inference_mode():\n",
        "    for batch in loader:\n",
        "        seed=batch[\"seed\"].to(\"cuda\")\n",
        "        gen_ids =model.generate(seed,max_new_tokens=32,do_sample=True,\n",
        "                           top_p=0.9,temperature=1.1,\n",
        "                           no_repeat_ngram_size=3,repetition_penalty=1.1,\n",
        "                           pad_token_id=tok.pad_token_id,\n",
        "                           eos_token_id=tok.eos_token_id)\n",
        "\n",
        "        for g_ids,p_ids in zip(batch[\"gold\"], gen.cpu()):\n",
        "            gold_tok=tok.convert_ids_to_tokens(g_ids.tolist())\n",
        "            pred_tok=[t for t in tok.convert_ids_to_tokens(\n",
        "                        p_ids.tolist(),skip_special_tokens=True)\n",
        "                      if t not in (\"<bos>\",\"<pad>\",\"<eos>\")][-32:]\n",
        "\n",
        "            update(gold_tok, gold, quick_key(gold_tok))\n",
        "            update(pred_tok, pred, quick_key(pred_tok))\n",
        "\n",
        "\n",
        "def avg(acc,key): return acc[key]/acc[f\"n_{key}\"] if acc[f\"n_{key}\"] else 0\n",
        "print(f\"{'Metric':6} | Gold | Pred\")\n",
        "print(\"-\"*28)\n",
        "for m in (\"cd\",\"cm\",\"ts\"):\n",
        "    print(f\"{m.upper():6} | {avg(gold,m):4.2f} | {avg(pred,m):4.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sEAgve7nROop",
        "outputId": "c6baf185-b4ec-44be-907e-a1a0a188672b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-b719fe4da52b>:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  s,g=self.items[i];return{\"seed\":torch.tensor(s),\"gold\":torch.tensor(g)}\n",
            "<ipython-input-10-b719fe4da52b>:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  s,g=self.items[i];return{\"seed\":torch.tensor(s),\"gold\":torch.tensor(g)}\n",
            "<ipython-input-10-b719fe4da52b>:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  s,g=self.items[i];return{\"seed\":torch.tensor(s),\"gold\":torch.tensor(g)}\n",
            "<ipython-input-10-b719fe4da52b>:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  s,g=self.items[i];return{\"seed\":torch.tensor(s),\"gold\":torch.tensor(g)}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Metric | Gold | Pred\n",
            "----------------------------\n",
            "CD     | 5.04 | 5.15\n",
            "CM     | 0.66 | 0.56\n",
            "TS     | 0.57 | 0.42\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Key Consistency"
      ],
      "metadata": {
        "id": "Nh5hIKfOVqmn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re, numpy as np, torch, music21 as m21\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AutoModelForCausalLM, PreTrainedTokenizerFast\n",
        "\n",
        "ROOT_RE = re.compile(r\"^([A-Ga-g])([#b♯♭]?)(.*)$\")\n",
        "PC_NUM  = {'C':0,'C#':1,'Db':1,'D':2,'D#':3,'Eb':3,'E':4,'F':5,'F#':6,'Gb':6,\n",
        "           'G':7,'G#':8,'Ab':8,'A':9,'A#':10,'Bb':10,'B':11,'Cb':11}\n",
        "\n",
        "def simplify(tok:str)->str|None:\n",
        "    \"\"\"\n",
        "    ตัดเหลือ root+quality หลัก ('' / m / dim)\n",
        "    Cmaj7(#11) → C,  F#m7 → F#m, Bdim7 → Bdim\n",
        "    \"\"\"\n",
        "    m = ROOT_RE.match(tok)\n",
        "    if not m: return None\n",
        "    root, acc, qual = m.groups()\n",
        "    root = root.upper() + acc.replace('♯','#').replace('♭','b')\n",
        "    qual = qual.lower()\n",
        "    if qual.startswith(('m','min')):   q = 'm'\n",
        "    elif 'dim' in qual or 'o' in qual: q = 'dim'\n",
        "    else:                              q = ''\n",
        "    return root + q\n",
        "\n",
        "def diatonic_set(key:m21.key.Key)->set[str]:\n",
        "    \"\"\"สร้างเซ็ต root+qual ('C','Dm','Edim',…) ที่อยู่ในคีย์\"\"\"\n",
        "    major_map = {1:'',2:'m',3:'m',4:'',5:'',6:'m',7:'dim'}\n",
        "    minor_map = {1:'m',2:'dim',3:'',4:'m',5:'m',6:'',7:''}\n",
        "    scale = (m21.scale.MajorScale if key.mode=='major'\n",
        "             else m21.scale.MinorScale)(key.tonic)\n",
        "    mapping = major_map if key.mode=='major' else minor_map\n",
        "    diat = set()\n",
        "    for deg,q in mapping.items():\n",
        "        root = scale.pitchFromDegree(deg).name.replace('-', 'b')\n",
        "        diat.add(root+q)\n",
        "    return diat\n",
        "\n",
        "def guess_key(chords:list[str])->m21.key.Key:\n",
        "    roots = [c[0].upper() for c in chords if c]\n",
        "    root  = max(set(roots), key=roots.count) if roots else 'C'\n",
        "    mode  = 'minor' if any(c.lower().endswith(('m','min')) for c in chords) else 'major'\n",
        "    return m21.key.Key(root, mode)\n",
        "\n",
        "seed_ds  = Seed4(test_ds,4,32)\n",
        "def coll(batch):\n",
        "    seeds=[b[\"seed\"] for b in batch]\n",
        "    golds=[b[\"gold\"] for b in batch]\n",
        "    seeds=torch.nn.utils.rnn.pad_sequence(\n",
        "        seeds,batch_first=True,padding_value=tok.pad_token_id)\n",
        "    return {\"seed\":seeds,\"gold\":golds}\n",
        "\n",
        "model_dir = \"drive/MyDrive/project-main/gpt2-chords-final\"\n",
        "tok   = PreTrainedTokenizerFast.from_pretrained(model_dir)\n",
        "tok.padding_side, tok.pad_token = \"left\",\"<pad>\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_dir).half().to(\"cuda\").eval()\n",
        "\n",
        "loader = DataLoader(seed_ds,batch_size=64,collate_fn=coll,\n",
        "                    shuffle=False,num_workers=4,pin_memory=True)\n",
        "\n",
        "gold_in = gold_all = pred_in = pred_all = 0  # ตัวนับ\n",
        "\n",
        "with torch.inference_mode():\n",
        "    for batch in loader:\n",
        "        seed = batch[\"seed\"].to(\"cuda\")\n",
        "        gen_ids =model.generate(seed,max_new_tokens=32,do_sample=True,\n",
        "                           top_p=0.9,temperature=1.1,\n",
        "                           no_repeat_ngram_size=3,repetition_penalty=1.1,\n",
        "                           pad_token_id=tok.pad_token_id,\n",
        "                           eos_token_id=tok.eos_token_id)\n",
        "\n",
        "        for g_ids,p_ids in zip(batch[\"gold\"], gen.cpu()):\n",
        "            gold_tok = tok.convert_ids_to_tokens(g_ids.tolist())\n",
        "            pred_tok = [t for t in tok.convert_ids_to_tokens(\n",
        "                          p_ids.tolist(),skip_special_tokens=True)\n",
        "                        if t not in (\"<bos>\",\"<pad>\",\"<eos>\")][-32:]\n",
        "\n",
        "            key_g   = guess_key(gold_tok)\n",
        "            diat_g  = diatonic_set(key_g)\n",
        "            for t in gold_tok:\n",
        "                simp = simplify(t)\n",
        "                if simp is None: continue\n",
        "                gold_all += 1\n",
        "                if simp in diat_g:\n",
        "                    gold_in += 1\n",
        "\n",
        "            key_p   = guess_key(pred_tok)\n",
        "            diat_p  = diatonic_set(key_p)\n",
        "            for t in pred_tok:\n",
        "                simp = simplify(t)\n",
        "                if simp is None: continue\n",
        "                pred_all += 1\n",
        "                if simp in diat_p:\n",
        "                    pred_in += 1\n",
        "\n",
        "kc_gold = gold_in / gold_all if gold_all else 0.0\n",
        "kc_pred = pred_in / pred_all if pred_all else 0.0\n",
        "print(f\"Key-Consistency  |  Gold = {kc_gold:.2%}   Pred = {kc_pred:.2%}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hbBk1AGpVqFW",
        "outputId": "58e1947c-0891-4ff3-89b2-d2187b794529"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-10-b719fe4da52b>:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  s,g=self.items[i];return{\"seed\":torch.tensor(s),\"gold\":torch.tensor(g)}\n",
            "<ipython-input-10-b719fe4da52b>:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  s,g=self.items[i];return{\"seed\":torch.tensor(s),\"gold\":torch.tensor(g)}\n",
            "<ipython-input-10-b719fe4da52b>:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  s,g=self.items[i];return{\"seed\":torch.tensor(s),\"gold\":torch.tensor(g)}\n",
            "<ipython-input-10-b719fe4da52b>:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  s,g=self.items[i];return{\"seed\":torch.tensor(s),\"gold\":torch.tensor(g)}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Key-Consistency  |  Gold = 36.30%   Pred = 69.23%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CHE (Chords Histrogram entropy\n",
        ")"
      ],
      "metadata": {
        "id": "yg9sS7aFZMGT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math, torch\n",
        "from torch.utils.data import DataLoader\n",
        "from collections import Counter\n",
        "from transformers import (AutoModelForCausalLM,\n",
        "                          PreTrainedTokenizerFast)\n",
        "\n",
        "seed_ds  = Seed4(test_ds, 4, 32)\n",
        "\n",
        "def collate(batch):\n",
        "    seeds=[b[\"seed\"] for b in batch]\n",
        "    golds=[b[\"gold\"] for b in batch]\n",
        "    seeds=torch.nn.utils.rnn.pad_sequence(\n",
        "        seeds, batch_first=True, padding_value=tok.pad_token_id)\n",
        "    return {\"seed\":seeds, \"gold\":golds}\n",
        "\n",
        "model_dir = \"drive/MyDrive/project-main/gpt2-chords-final\"\n",
        "tok   = PreTrainedTokenizerFast.from_pretrained(model_dir)\n",
        "tok.padding_side, tok.pad_token = \"left\",\"<pad>\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_dir).half().to(\"cuda\").eval()\n",
        "\n",
        "loader = DataLoader(seed_ds, batch_size=64, collate_fn=collate,\n",
        "                    shuffle=False, num_workers=4, pin_memory=True)\n",
        "\n",
        "gold_counter = Counter()\n",
        "pred_counter = Counter()\n",
        "gold_total = pred_total = 0\n",
        "\n",
        "with torch.inference_mode():\n",
        "    for batch in loader:\n",
        "        seed = batch[\"seed\"].to(\"cuda\")\n",
        "        gen = model.generate(seed,max_new_tokens=32,do_sample=True,\n",
        "                           top_p=0.9,temperature=1.1,\n",
        "                           no_repeat_ngram_size=3,repetition_penalty=1.1,\n",
        "                           pad_token_id=tok.pad_token_id,\n",
        "                           eos_token_id=tok.eos_token_id)\n",
        "\n",
        "        for g_ids, p_ids in zip(batch[\"gold\"], gen.cpu()):\n",
        "            gold_tok = tok.convert_ids_to_tokens(g_ids.tolist())\n",
        "            gold_counter.update(gold_tok);  gold_total += len(gold_tok)\n",
        "\n",
        "            pred_tok = [t for t in tok.convert_ids_to_tokens(\n",
        "                          p_ids.tolist(), skip_special_tokens=True)\n",
        "                        if t not in (\"<bos>\",\"<pad>\",\"<eos>\")][-32:]\n",
        "            pred_counter.update(pred_tok);  pred_total += len(pred_tok)\n",
        "\n",
        "def entropy(counter, total):\n",
        "    return -sum((c/total) * math.log2(c/total)\n",
        "                for c in counter.values() if c)\n",
        "\n",
        "che_gold = entropy(gold_counter, gold_total)\n",
        "che_pred = entropy(pred_counter, pred_total)\n",
        "\n",
        "print(f\"Chord-Histogram Entropy\")\n",
        "print(f\"  Gold-32 : {che_gold:.3f} bits\")\n",
        "print(f\"  Pred-32 : {che_pred:.3f} bits\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QZlMWIzmZRIS",
        "outputId": "77505285-6cec-43d6-d01b-56abdec8af3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "<ipython-input-10-b719fe4da52b>:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  s,g=self.items[i];return{\"seed\":torch.tensor(s),\"gold\":torch.tensor(g)}\n",
            "<ipython-input-10-b719fe4da52b>:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  s,g=self.items[i];return{\"seed\":torch.tensor(s),\"gold\":torch.tensor(g)}\n",
            "<ipython-input-10-b719fe4da52b>:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  s,g=self.items[i];return{\"seed\":torch.tensor(s),\"gold\":torch.tensor(g)}\n",
            "<ipython-input-10-b719fe4da52b>:56: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  s,g=self.items[i];return{\"seed\":torch.tensor(s),\"gold\":torch.tensor(g)}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chord-Histogram Entropy\n",
            "  Gold-32 : 5.039 bits\n",
            "  Pred-32 : 6.863 bits\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "dockerImageVersionId": 31012,
      "isGpuEnabled": false,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}